{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63d8206e",
   "metadata": {},
   "source": [
    "# Preprocessing Experiments: xLSTM Paper -style Light vs Aggressive Cleaning\n",
    "\n",
    "This notebook implements and compares two text preprocessing pipelines:\n",
    "\n",
    "- **Light preprocessing (xLSTM-inspired)**: minimal normalization consistent with the xLSTM paper.\n",
    "- **Aggressive preprocessing**: stronger normalization (legacy pipeline) aimed at reducing noise but potentially removing useful signals.\n",
    "\n",
    "We export both processed datasets to `data/processed/` for downstream models (TF-IDF, Naive Bayes, Transformer).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5899bb0",
   "metadata": {},
   "source": [
    "## Why preprocessing?\n",
    "\n",
    "Preprocessing can:\n",
    "- reduce noise (URLs, emails, repeated chars, formatting artifacts),\n",
    "- normalize lexical variants (lemmatization),\n",
    "- reduce vocabulary size (stopword removal),\n",
    "- improve generalization for classical models (TF-IDF / Naive Bayes).\n",
    "\n",
    "However, aggressive cleaning may also remove predictive signals (e.g., slang, misspellings, emoji cues).\n",
    "Therefore we compare **light vs aggressive** preprocessing as an ablation study.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a02a68ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\envs\\advanced_env_py311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\advanced_ml\\2nd_pull\\Toxic-Comment-Classification-using-Classical-NLP-Methods-and-a-Lightweight-Transformer\\notebooks\\../src\\preprocessing.py:7: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources as pkgr\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# NLP tools for light preprocessing\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import our preprocessing functions\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from preprocessing import preprocess_light_xlstm, preprocess, preprocess_light_xlstm_from_doc, preprocess_from_doc # adjust name if using preprocess_aggressive\n",
    "\n",
    "# Initialize spaCy and stopword list\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Paths for saving processed datasets\n",
    "processed_dir = Path(\"../data/processed\")\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291a0b46",
   "metadata": {},
   "source": [
    "This cell loads the required libraries, initializes the spaCy model and NLTK stopwords, and imports the two preprocessing functions.  \n",
    "- `preprocess_light_xlstm()` implements the light pipeline: lowercasing, URL/email removal, number replacement (`<NUM>`), lemmatization, stopword removal, and whitespace normalization.  \n",
    "- `preprocess()` (or `preprocess_aggressive()`) implements the aggressive cleaning pipeline already coded in `src/preprocessing.py`.\n",
    "\n",
    "We also ensure that a `data/processed/` directory exists for saving the cleaned datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10b17604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (127656, 7)\n",
      "Validation size: (15957, 7)\n",
      "Test size: (15958, 7)\n",
      "127656 15957 15958\n"
     ]
    }
   ],
   "source": [
    "# Load Civil Comments dataset\n",
    "dataset = load_dataset(\"thesofakillers/jigsaw-toxic-comment-classification-challenge\")  #google/civil_comments\n",
    "\n",
    "# train (80%) / validation(10%) / test(10%) split\n",
    "train_test = dataset[\"train\"].train_test_split(\n",
    "    test_size=0.2,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "valid_test = train_test[\"test\"].train_test_split(\n",
    "    test_size=0.5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "train_ds = train_test[\"train\"]\n",
    "valid_ds = valid_test[\"train\"]\n",
    "test_ds  = valid_test[\"test\"]\n",
    "\n",
    "\n",
    "# Convert to Pandas for easier manipulation\n",
    "df_train = train_ds.to_pandas().rename(columns={\"comment_text\": \"text\"}).drop(columns=[\"id\"])\n",
    "df_valid = valid_ds.to_pandas().rename(columns={\"comment_text\": \"text\"}).drop(columns=[\"id\"])\n",
    "df_test  = test_ds.to_pandas().rename(columns={\"comment_text\": \"text\"}).drop(columns=[\"id\"])\n",
    "\n",
    "# Show shapes\n",
    "print(\"Train size:\", df_train.shape)\n",
    "print(\"Validation size:\", df_valid.shape)\n",
    "print(\"Test size:\", df_test.shape)\n",
    "\n",
    "# --- Subsample for development ---\n",
    "SUBSAMPLE_FRAC = 1  # 100%\n",
    "\n",
    "df_train_small = df_train.sample(frac=SUBSAMPLE_FRAC, random_state=42).copy()\n",
    "df_valid_small = df_valid.sample(frac=SUBSAMPLE_FRAC, random_state=42).copy()\n",
    "df_test_small  = df_test.sample(frac=SUBSAMPLE_FRAC, random_state=42).copy()\n",
    "\n",
    "print(len(df_train_small), len(df_valid_small), len(df_test_small))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1e48e0",
   "metadata": {},
   "source": [
    "## Light preprocessing (xLSTM-inspired)\n",
    "\n",
    "Steps:\n",
    "1. Lowercase\n",
    "2. Remove URLs and emails\n",
    "3. Replace numeric sequences with `<NUM>`\n",
    "4. Lemmatization with spaCy\n",
    "5. Stopword removal with NLTK\n",
    "6. Whitespace normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ee641f",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "\n",
    "- preserves sentence structure and many important signals (e.g. obfuscated insults, writing style)\n",
    "\n",
    "- often better suited for Transformers and models capable of learning complex patterns\n",
    "\n",
    "- lower risk of over-cleaning the text\n",
    "\n",
    "Drawbacks:\n",
    "\n",
    "- larger vocabulary, which can lead to higher sparsity for TF-IDF or Naive Bayes models\n",
    "\n",
    "- retains more noise (typos, repetitions, emojis, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199782b8",
   "metadata": {},
   "source": [
    "## Aggressive preprocessing\n",
    "\n",
    "This pipeline applies stronger normalization (e.g., slang expansion, emoji conversion, spelling correction, etc.).\n",
    "It may reduce noise further but can also discard useful information.\n",
    "\n",
    "We use it as a comparison baseline to quantify how much cleaning is beneficial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d22bc8c",
   "metadata": {},
   "source": [
    "Implemented steps:\n",
    "\n",
    "- lowercasing\n",
    "\n",
    "- removal of URLs and email addresses\n",
    "\n",
    "- Unicode and accent normalization\n",
    "\n",
    "- Removal of HTML artifacts\n",
    "\n",
    "- removal of punctuation and special characters\n",
    "\n",
    "- normalization of excessive character repetitions (e.g. sooooo â†’ soo)\n",
    "\n",
    "- stopword removal\n",
    "\n",
    "- lemmatization\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- strongly reduces vocabulary size\n",
    "\n",
    "- beneficial for Naive Bayes and TF-IDF models by reducing sparsity\n",
    "\n",
    "- produces more standardized and stable text representations\n",
    "\n",
    "Drawbacks:\n",
    "\n",
    "- may remove subtle toxic signals such as creative spellings or obfuscations\n",
    "\n",
    "- can overly normalize the original text and reduce stylistic information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8be7bd5",
   "metadata": {},
   "source": [
    "## Apply preprocessing and compare\n",
    "\n",
    "We compare:\n",
    "- average token length (words),\n",
    "- fraction of empty outputs,\n",
    "- vocabulary size proxy,\n",
    "- qualitative examples (same raw text processed by both pipelines).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8505fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import preprocess_light_xlstm, preprocess\n",
    "\n",
    "def light(text):\n",
    "    return preprocess_light_xlstm(text, nlp=nlp, stopwords=stop_words)\n",
    "\n",
    "def aggressive(text):\n",
    "    return preprocess(text, nlp=nlp, stopwords=stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e12419e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_light</th>\n",
       "      <th>text_aggressive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>89061</th>\n",
       "      <td>That would be welcome. I don't seem to have an...</td>\n",
       "      <td>would welcome . I seem anything would pd . one...</td>\n",
       "      <td>would welcome I seem anything would pd one ima...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29562</th>\n",
       "      <td>, You taking money from /r/gamerghazi and cont...</td>\n",
       "      <td>, take money /r / gamerghazi continue edit gam...</td>\n",
       "      <td>take money r gamerghazi continue edit gamergat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3615</th>\n",
       "      <td>ROMANIAN-AMERICANS, AGAIN YOU!!!!\\nWhat is aga...</td>\n",
       "      <td>romanian - americans , ! ! ! ! problem list ro...</td>\n",
       "      <td>romanian americans problem list romanian ameri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27958</th>\n",
       "      <td>you're a tyrant \\n\\nyou're only deleting my ed...</td>\n",
       "      <td>tyrant delete edit angelique 's surname carrin...</td>\n",
       "      <td>tyrant delete edit angelique s surname carring...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118482</th>\n",
       "      <td>\"Hi Paul\\n\\nGiven your previous edits of this ...</td>\n",
       "      <td>\" hi paul give previous edit article I wary ne...</td>\n",
       "      <td>hi paul give previous edit article I wary neut...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "89061   That would be welcome. I don't seem to have an...   \n",
       "29562   , You taking money from /r/gamerghazi and cont...   \n",
       "3615    ROMANIAN-AMERICANS, AGAIN YOU!!!!\\nWhat is aga...   \n",
       "27958   you're a tyrant \\n\\nyou're only deleting my ed...   \n",
       "118482  \"Hi Paul\\n\\nGiven your previous edits of this ...   \n",
       "\n",
       "                                               text_light  \\\n",
       "89061   would welcome . I seem anything would pd . one...   \n",
       "29562   , take money /r / gamerghazi continue edit gam...   \n",
       "3615    romanian - americans , ! ! ! ! problem list ro...   \n",
       "27958   tyrant delete edit angelique 's surname carrin...   \n",
       "118482  \" hi paul give previous edit article I wary ne...   \n",
       "\n",
       "                                          text_aggressive  \n",
       "89061   would welcome I seem anything would pd one ima...  \n",
       "29562   take money r gamerghazi continue edit gamergat...  \n",
       "3615    romanian americans problem list romanian ameri...  \n",
       "27958   tyrant delete edit angelique s surname carring...  \n",
       "118482  hi paul give previous edit article I wary neut...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Sanity check: very small sample ---\n",
    "N_SANITY = 100\n",
    "\n",
    "df_sanity = df_train.sample(N_SANITY, random_state=42).copy()\n",
    "\n",
    "df_sanity[\"text_light\"] = df_sanity[\"text\"].apply(light)\n",
    "df_sanity[\"text_aggressive\"] = df_sanity[\"text\"].apply(aggressive)\n",
    "\n",
    "df_sanity[[\"text\", \"text_light\", \"text_aggressive\"]].head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c03fbc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preprocessing train set.\n",
      "Finished preprocessing validation set.\n",
      "Finished preprocessing test set.\n"
     ]
    }
   ],
   "source": [
    "### --- Apply light preprocessing to datasets --- (running time : 25 minutes) \n",
    "## Train set\n",
    "docs = nlp.pipe(\n",
    "    df_train_small[\"text\"].tolist(),\n",
    "    batch_size=512,\n",
    "    n_process=4\n",
    ")\n",
    "\n",
    "df_train_small[\"text_light\"] = [\n",
    "    preprocess_light_xlstm_from_doc(doc, stop_words)\n",
    "    for doc in docs\n",
    "]\n",
    "\n",
    "print(\"Finished preprocessing train set.\")\n",
    "\n",
    "\n",
    "## Validation set\n",
    "docs = nlp.pipe(\n",
    "    df_valid_small[\"text\"].tolist(),\n",
    "    batch_size=512,\n",
    "    n_process=4\n",
    ")\n",
    "\n",
    "df_valid_small[\"text_light\"] = [\n",
    "    preprocess_light_xlstm_from_doc(doc, stop_words)\n",
    "    for doc in docs\n",
    "]\n",
    "\n",
    "print(\"Finished preprocessing validation set.\")\n",
    "\n",
    "\n",
    "## Test set\n",
    "docs = nlp.pipe(\n",
    "    df_test_small[\"text\"].tolist(),\n",
    "    batch_size=512,\n",
    "    n_process=4\n",
    ")\n",
    "\n",
    "df_test_small[\"text_light\"] = [\n",
    "    preprocess_light_xlstm_from_doc(doc, stop_words)\n",
    "    for doc in docs\n",
    "]\n",
    "\n",
    "print(\"Finished preprocessing test set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2153649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preprocessing train set.\n",
      "Finished preprocessing validation set.\n",
      "Finished preprocessing test set.\n"
     ]
    }
   ],
   "source": [
    "### --- Apply aggressive preprocessing to datasets ---  (running time : 27 minutes)\n",
    "## Train set\n",
    "docs = nlp.pipe(\n",
    "    df_train_small[\"text\"].tolist(),\n",
    "    batch_size=512,\n",
    "    n_process=4\n",
    ")\n",
    "\n",
    "df_train_small[\"text_aggressive\"] = [\n",
    "    preprocess_from_doc(doc, stop_words)\n",
    "    for doc in docs\n",
    "]\n",
    "\n",
    "print(\"Finished preprocessing train set.\")\n",
    "\n",
    "\n",
    "## Validation set\n",
    "docs = nlp.pipe(\n",
    "    df_valid_small[\"text\"].tolist(),\n",
    "    batch_size=512,\n",
    "    n_process=4\n",
    ")\n",
    "\n",
    "df_valid_small[\"text_aggressive\"] = [\n",
    "    preprocess_from_doc(doc, stop_words)\n",
    "    for doc in docs\n",
    "]\n",
    "\n",
    "print(\"Finished preprocessing validation set.\")\n",
    "\n",
    "\n",
    "## Test set\n",
    "docs = nlp.pipe(\n",
    "    df_test_small[\"text\"].tolist(),\n",
    "    batch_size=512,\n",
    "    n_process=4\n",
    ")\n",
    "\n",
    "df_test_small[\"text_aggressive\"] = [\n",
    "    preprocess_from_doc(doc, stop_words)\n",
    "    for doc in docs\n",
    "]\n",
    "\n",
    "print(\"Finished preprocessing test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2cdac2",
   "metadata": {},
   "source": [
    "## Export processed datasets\n",
    "\n",
    "We export two versions:\n",
    "- `civil_comments_light_xlstm.(csv|parquet)`\n",
    "- `civil_comments_aggressive.(csv|parquet)`\n",
    "\n",
    "They are stored in `data/processed/` and excluded from Git versioning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5078a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_small.to_parquet(\"../data/processed/train_cleaned.parquet\", index=False)\n",
    "df_valid_small.to_parquet(\"../data/processed/valid_cleaned.parquet\", index=False)\n",
    "df_test_small.to_parquet(\"../data/processed/test_cleaned.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b358304",
   "metadata": {},
   "source": [
    "## Comparison of preprocessing strategies\n",
    "\n",
    "We compare the two pipelines using text length statistics and qualitative examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7456f1ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>len_light</th>\n",
       "      <th>len_aggr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>127656.000000</td>\n",
       "      <td>127656.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>49.585801</td>\n",
       "      <td>37.934629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>77.583008</td>\n",
       "      <td>58.908626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>26.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>54.000000</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4568.000000</td>\n",
       "      <td>1381.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           len_light       len_aggr\n",
       "count  127656.000000  127656.000000\n",
       "mean       49.585801      37.934629\n",
       "std        77.583008      58.908626\n",
       "min         1.000000       0.000000\n",
       "25%        13.000000      10.000000\n",
       "50%        26.000000      20.000000\n",
       "75%        54.000000      41.000000\n",
       "max      4568.000000    1381.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_small[\"len_light\"] = df_train_small[\"text_light\"].str.split().apply(len)\n",
    "df_train_small[\"len_aggr\"]  = df_train_small[\"text_aggressive\"].str.split().apply(len)\n",
    "\n",
    "df_train_small[[\"len_light\", \"len_aggr\"]].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d67c9c",
   "metadata": {},
   "source": [
    "## Qualitative comparison: raw vs light vs aggressive preprocessing\n",
    "\n",
    "To better understand the impact of preprocessing, we display a concrete example\n",
    "of a raw comment alongside its light (xLSTM-style) and aggressive cleaned versions.\n",
    "This qualitative comparison highlights how different preprocessing choices\n",
    "transform the same input text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3de1e341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW COMMENT:\n",
      "Missing Champions \n",
      "\n",
      "This article should have ALL the TNA World Heavyweight Champions, from Ken Shamrock. Weird that it doesn't.\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "LIGHT PREPROCESSING (xLSTM-style):\n",
      "miss champion article tna world heavyweight champion , ken shamrock . weird .\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "AGGRESSIVE PREPROCESSING:\n",
      "miss champion article tna world heavyweight champion ken shamrock weird\n"
     ]
    }
   ],
   "source": [
    "# Select one example comment\n",
    "example_text = df_train.loc[0, \"text\"]\n",
    "\n",
    "# Apply both preprocessing pipelines\n",
    "example_light = light(example_text)\n",
    "example_aggressive = aggressive(example_text)\n",
    "\n",
    "print(\"RAW COMMENT:\")\n",
    "print(example_text)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100 + \"\\n\")\n",
    "\n",
    "print(\"LIGHT PREPROCESSING (xLSTM-style):\")\n",
    "print(example_light)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100 + \"\\n\")\n",
    "\n",
    "print(\"AGGRESSIVE PREPROCESSING:\")\n",
    "print(example_aggressive)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da374772",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook compared light and aggressive preprocessing strategies.\n",
    "The resulting datasets will be used to train and evaluate multiple models,\n",
    "allowing us to assess the impact of preprocessing choices on performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
