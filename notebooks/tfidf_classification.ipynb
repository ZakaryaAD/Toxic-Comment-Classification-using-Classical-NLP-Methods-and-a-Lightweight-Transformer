{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e0a7826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e88838",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84b1fa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_parquet(\"../data/processed/train_cleaned.parquet\")\n",
    "df_valid = pd.read_parquet(\"../data/processed/valid_cleaned.parquet\")\n",
    "df_test  = pd.read_parquet(\"../data/processed/test_cleaned.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e02f8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_COL = \"text_aggressive\"\n",
    "\n",
    "X_train = df_train[TEXT_COL]\n",
    "X_valid = df_valid[TEXT_COL]\n",
    "X_test  = df_test[TEXT_COL]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c7a2fb",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6215061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=10000,\n",
    "    min_df=5,\n",
    "    max_df=0.95,\n",
    "    sublinear_tf=True,\n",
    "    norm=\"l2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a7d2039",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_valid_tfidf = tfidf.transform(X_valid)\n",
    "X_test_tfidf  = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9630b4c3",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68578e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scale_pos_weight(y: pd.Series) -> float:\n",
    "    n_pos = (y == 1).sum()\n",
    "    n_neg = (y == 0).sum()\n",
    "    return n_neg / max(n_pos, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a9af933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training toxic\n",
      "Training severe_toxic\n",
      "Training obscene\n",
      "Training threat\n",
      "Training insult\n",
      "Training identity_hate\n"
     ]
    }
   ],
   "source": [
    "LABELS = [\n",
    "    \"toxic\",\n",
    "    \"severe_toxic\",\n",
    "    \"obscene\",\n",
    "    \"threat\",\n",
    "    \"insult\",\n",
    "    \"identity_hate\"\n",
    "]\n",
    "\n",
    "models = {}\n",
    "\n",
    "for label in LABELS:\n",
    "    print(f\"Training {label}\")\n",
    "\n",
    "    y_train = df_train[label]\n",
    "    y_valid = df_valid[label]\n",
    "\n",
    "    spw = compute_scale_pos_weight(y_train)\n",
    "\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"logloss\",\n",
    "        scale_pos_weight=spw,\n",
    "        tree_method=\"hist\",\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train_tfidf,\n",
    "        y_train,\n",
    "        eval_set=[(X_valid_tfidf, y_valid)],\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    models[label] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628ba59b",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b9943a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_multilabel(models, X):\n",
    "    y_pred = {}\n",
    "    y_proba = {}\n",
    "\n",
    "    for label, model in models.items():\n",
    "        y_proba[label] = model.predict_proba(X)[:, 1]\n",
    "        y_pred[label] = (y_proba[label] >= 0.5).astype(int)\n",
    "\n",
    "    return y_pred, y_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55518b23",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b765492e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    hamming_loss,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "def evaluate_multilabel(\n",
    "    y_true_df,\n",
    "    y_pred_dict,\n",
    "    labels,\n",
    "    split_name=\"test\",\n",
    "    output_path=\"multilabel_results.xlsx\"\n",
    "):\n",
    "    Y_true = y_true_df[labels].values\n",
    "    Y_pred = np.column_stack([y_pred_dict[l] for l in labels])\n",
    "\n",
    "    # ---------- Global metrics ----------\n",
    "    global_metrics = {\n",
    "        \"split\": split_name,\n",
    "        \"macro_f1\": f1_score(Y_true, Y_pred, average=\"macro\", zero_division=0),\n",
    "        \"micro_f1\": f1_score(Y_true, Y_pred, average=\"micro\", zero_division=0),\n",
    "        \"exact_accuracy\": accuracy_score(Y_true, Y_pred),\n",
    "        \"hamming_loss\": hamming_loss(Y_true, Y_pred),\n",
    "    }\n",
    "\n",
    "    df_global = pd.DataFrame([global_metrics])\n",
    "\n",
    "    # ---------- Per-label detailed metrics ----------\n",
    "    rows = []\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        report = classification_report(\n",
    "            Y_true[:, i],\n",
    "            Y_pred[:, i],\n",
    "            output_dict=True,\n",
    "            zero_division=0\n",
    "        )\n",
    "\n",
    "        for cls, metrics in report.items():\n",
    "            if cls == \"accuracy\":\n",
    "                continue\n",
    "\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"split\": split_name,\n",
    "                    \"label\": label,\n",
    "                    \"class\": cls,\n",
    "                    \"precision\": metrics.get(\"precision\"),\n",
    "                    \"recall\": metrics.get(\"recall\"),\n",
    "                    \"f1_score\": metrics.get(\"f1-score\"),\n",
    "                    \"support\": metrics.get(\"support\"),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    df_details = pd.DataFrame(rows)\n",
    "\n",
    "    # ---------- Save to Excel (no explicit engine) ----------\n",
    "    with pd.ExcelWriter(output_path) as writer:\n",
    "        df_global.to_excel(writer, sheet_name=\"global_metrics\", index=False)\n",
    "        df_details.to_excel(writer, sheet_name=\"per_label_metrics\", index=False)\n",
    "\n",
    "    print(f\"Results saved to {output_path}\")\n",
    "\n",
    "    return df_global, df_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd6a360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ../data/results/results_train.xlsx\n",
      "Results saved to ../data/results/results_valid.xlsx\n",
      "Results saved to ../data/results/results_test.xlsx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(  split  macro_f1  micro_f1  exact_accuracy  hamming_loss\n",
       " 0  test  0.485321  0.620728        0.869909      0.035468,\n",
       "    split          label         class  precision    recall  f1_score  support\n",
       " 0   test          toxic             0   0.973178  0.961565  0.967337  14414.0\n",
       " 1   test          toxic             1   0.677156  0.752591  0.712883   1544.0\n",
       " 2   test          toxic     macro avg   0.825167  0.857078  0.840110  15958.0\n",
       " 3   test          toxic  weighted avg   0.944537  0.941346  0.942717  15958.0\n",
       " 4   test   severe_toxic             0   0.997854  0.970767  0.984124  15804.0\n",
       " 5   test   severe_toxic             1   0.207547  0.785714  0.328358    154.0\n",
       " 6   test   severe_toxic     macro avg   0.602700  0.878241  0.656241  15958.0\n",
       " 7   test   severe_toxic  weighted avg   0.990227  0.968981  0.977796  15958.0\n",
       " 8   test        obscene             0   0.992826  0.978590  0.985657  15133.0\n",
       " 9   test        obscene             1   0.689060  0.870303  0.769148    825.0\n",
       " 10  test        obscene     macro avg   0.840943  0.924446  0.877403  15958.0\n",
       " 11  test        obscene  weighted avg   0.977122  0.972992  0.974464  15958.0\n",
       " 12  test         threat             0   0.999359  0.980146  0.989659  15916.0\n",
       " 13  test         threat             1   0.091954  0.761905  0.164103     42.0\n",
       " 14  test         threat     macro avg   0.545657  0.871025  0.576881  15958.0\n",
       " 15  test         threat  weighted avg   0.996971  0.979571  0.987487  15958.0\n",
       " 16  test         insult             0   0.989998  0.965720  0.977708  15169.0\n",
       " 17  test         insult             1   0.552110  0.812421  0.657436    789.0\n",
       " 18  test         insult     macro avg   0.771054  0.889070  0.817572  15958.0\n",
       " 19  test         insult  weighted avg   0.968348  0.958140  0.961873  15958.0\n",
       " 20  test  identity_hate             0   0.997980  0.967830  0.982673  15822.0\n",
       " 21  test  identity_hate             1   0.171010  0.772059  0.280000    136.0\n",
       " 22  test  identity_hate     macro avg   0.584495  0.869944  0.631337  15958.0\n",
       " 23  test  identity_hate  weighted avg   0.990932  0.966161  0.976685  15958.0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRAIN\n",
    "y_pred_train, _ = predict_multilabel(models, X_train_tfidf)\n",
    "evaluate_multilabel(df_train, y_pred_train, LABELS, \"train\", \"../reports/results/results_train.xlsx\")\n",
    "\n",
    "# VALID\n",
    "y_pred_valid, _ = predict_multilabel(models, X_valid_tfidf)\n",
    "evaluate_multilabel(df_valid, y_pred_valid, LABELS, \"valid\", \"../reports/results/results_valid.xlsx\")  \n",
    "# TEST\n",
    "y_pred_test, _ = predict_multilabel(models, X_test_tfidf)\n",
    "evaluate_multilabel(df_test, y_pred_test, LABELS, \"test\", \"../reports/results/results_test.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
