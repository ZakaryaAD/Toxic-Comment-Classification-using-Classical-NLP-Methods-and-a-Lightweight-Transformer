{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63d8206e",
   "metadata": {},
   "source": [
    "# Preprocessing Experiments: xLSTM Paper -style Light vs Aggressive Cleaning\n",
    "\n",
    "This notebook implements and compares two text preprocessing pipelines:\n",
    "\n",
    "- **Light preprocessing (xLSTM-inspired)**: minimal normalization consistent with the xLSTM paper.\n",
    "- **Aggressive preprocessing**: stronger normalization (legacy pipeline) aimed at reducing noise but potentially removing useful signals.\n",
    "\n",
    "We export both processed datasets to `data/processed/` for downstream models (TF-IDF, Naive Bayes, Transformer).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5899bb0",
   "metadata": {},
   "source": [
    "## Why preprocessing?\n",
    "\n",
    "Preprocessing can:\n",
    "- reduce noise (URLs, emails, repeated chars, formatting artifacts),\n",
    "- normalize lexical variants (lemmatization),\n",
    "- reduce vocabulary size (stopword removal),\n",
    "- improve generalization for classical models (TF-IDF / Naive Bayes).\n",
    "\n",
    "However, aggressive cleaning may also remove predictive signals (e.g., slang, misspellings, emoji cues).\n",
    "Therefore we compare **light vs aggressive** preprocessing as an ablation study.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a02a68ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# NLP tools for light preprocessing\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import our preprocessing functions\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from preprocessing import preprocess_light_xlstm, preprocess  # adjust name if using preprocess_aggressive\n",
    "\n",
    "# Initialize spaCy and stopword list\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Paths for saving processed datasets\n",
    "processed_dir = Path(\"../data/processed\")\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291a0b46",
   "metadata": {},
   "source": [
    "This cell loads the required libraries, initializes the spaCy model and NLTK stopwords, and imports the two preprocessing functions.  \n",
    "- `preprocess_light_xlstm()` implements the light pipeline: lowercasing, URL/email removal, number replacement (`<NUM>`), lemmatization, stopword removal, and whitespace normalization.  \n",
    "- `preprocess()` (or `preprocess_aggressive()`) implements the aggressive cleaning pipeline already coded in `src/preprocessing.py`.\n",
    "\n",
    "We also ensure that a `data/processed/` directory exists for saving the cleaned datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b17604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (1804874, 8)\n",
      "Validation size: (97320, 8)\n",
      "Test size: (97320, 8)\n",
      "180487 9732 9732\n"
     ]
    }
   ],
   "source": [
    "# Load Civil Comments dataset\n",
    "dataset = load_dataset(\"google/civil_comments\")\n",
    "train_ds = dataset[\"train\"]\n",
    "valid_ds = dataset[\"validation\"]\n",
    "test_ds  = dataset[\"test\"]\n",
    "\n",
    "# Convert to Pandas for easier manipulation\n",
    "df_train = train_ds.to_pandas()\n",
    "df_valid = valid_ds.to_pandas()\n",
    "df_test  = test_ds.to_pandas()\n",
    "\n",
    "# Show shapes\n",
    "print(\"Train size:\", df_train.shape)\n",
    "print(\"Validation size:\", df_valid.shape)\n",
    "print(\"Test size:\", df_test.shape)\n",
    "\n",
    "# --- Subsample for development ---\n",
    "SUBSAMPLE_FRAC = 0.1  # 10%\n",
    "\n",
    "df_train_small = df_train.sample(frac=SUBSAMPLE_FRAC, random_state=42).copy()\n",
    "df_valid_small = df_valid.sample(frac=SUBSAMPLE_FRAC, random_state=42).copy()\n",
    "df_test_small  = df_test.sample(frac=SUBSAMPLE_FRAC, random_state=42).copy()\n",
    "\n",
    "print(len(df_train_small), len(df_valid_small), len(df_test_small))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6e0d1b",
   "metadata": {},
   "source": [
    "## Target definition\n",
    "\n",
    "Civil Comments labels are continuous scores in [0, 1].\n",
    "For classification baselines, we convert them to binary targets using a threshold `THRESH`.\n",
    "\n",
    "We will:\n",
    "- create binary labels for each toxicity dimension,\n",
    "- optionally create a global label `is_toxic = 1(toxicity > THRESH)` for single-label baselines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71c2cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESH = 0.5\n",
    "label_cols = [\n",
    "    \"toxicity\", \"severe_toxicity\", \"obscene\",\n",
    "    \"threat\", \"insult\", \"identity_attack\"\n",
    "]\n",
    "\n",
    "for col in label_cols:\n",
    "    df_train_small[col + \"_bin\"] = (df_train_small[col] > THRESH).astype(int)\n",
    "    df_valid_small[col + \"_bin\"] = (df_valid_small[col] > THRESH).astype(int)\n",
    "    df_test_small[col + \"_bin\"]  = (df_test_small[col] > THRESH).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1e48e0",
   "metadata": {},
   "source": [
    "## Light preprocessing (xLSTM-inspired)\n",
    "\n",
    "Steps:\n",
    "1. Lowercase\n",
    "2. Remove URLs and emails\n",
    "3. Replace numeric sequences with `<NUM>`\n",
    "4. Lemmatization with spaCy\n",
    "5. Stopword removal with NLTK\n",
    "6. Whitespace normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ee641f",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "\n",
    "- preserves sentence structure and many important signals (e.g. obfuscated insults, writing style)\n",
    "\n",
    "- often better suited for Transformers and models capable of learning complex patterns\n",
    "\n",
    "- lower risk of over-cleaning the text\n",
    "\n",
    "Drawbacks:\n",
    "\n",
    "- larger vocabulary, which can lead to higher sparsity for TF-IDF or Naive Bayes models\n",
    "\n",
    "- retains more noise (typos, repetitions, emojis, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199782b8",
   "metadata": {},
   "source": [
    "## Aggressive preprocessing (legacy)\n",
    "\n",
    "This pipeline applies stronger normalization (e.g., slang expansion, emoji conversion, spelling correction, etc.).\n",
    "It may reduce noise further but can also discard useful information.\n",
    "\n",
    "We use it as a comparison baseline to quantify how much cleaning is beneficial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d22bc8c",
   "metadata": {},
   "source": [
    "Implemented steps:\n",
    "\n",
    "- lowercasing\n",
    "\n",
    "- removal of HTML tags and HTML entities\n",
    "\n",
    "- removal of URLs and email addresses\n",
    "\n",
    "- Unicode and accent normalization\n",
    "\n",
    "- removal of punctuation and special characters\n",
    "\n",
    "- normalization of excessive character repetitions (e.g. sooooo â†’ soo)\n",
    "\n",
    "- stopword removal\n",
    "\n",
    "- lemmatization\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- strongly reduces vocabulary size\n",
    "\n",
    "- beneficial for Naive Bayes and TF-IDF models by reducing sparsity\n",
    "\n",
    "- produces more standardized and stable text representations\n",
    "\n",
    "Drawbacks:\n",
    "\n",
    "- may remove subtle toxic signals such as creative spellings or obfuscations\n",
    "\n",
    "- can overly normalize the original text and reduce stylistic information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8be7bd5",
   "metadata": {},
   "source": [
    "## Apply preprocessing and compare\n",
    "\n",
    "We compare:\n",
    "- average token length (words),\n",
    "- fraction of empty outputs,\n",
    "- vocabulary size proxy,\n",
    "- qualitative examples (same raw text processed by both pipelines).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8505fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import preprocess_light_xlstm, preprocess\n",
    "\n",
    "def light(text):\n",
    "    return preprocess_light_xlstm(text, nlp=nlp, stopwords=stop_words)\n",
    "\n",
    "def aggressive(text):\n",
    "    return preprocess(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e12419e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_light</th>\n",
       "      <th>text_aggressive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>286892</th>\n",
       "      <td>What a breathe of fresh air to have someone wh...</td>\n",
       "      <td>breathe fresh air someone embrace common sense...</td>\n",
       "      <td>breathe fresh air someone embrace common sense...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419218</th>\n",
       "      <td>Your jewish friends were the ones who told you...</td>\n",
       "      <td>jewish friend one tell zionist control canada ...</td>\n",
       "      <td>jewish friend one tell zionists control canada...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055330</th>\n",
       "      <td>Possible collusion by Trump and his affiliates...</td>\n",
       "      <td>possible collusion trump affiliate debunk , st...</td>\n",
       "      <td>possible collusion trump affiliate debunk stat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1382764</th>\n",
       "      <td>Exactly.  We need a % of GDP spending cap at t...</td>\n",
       "      <td>exactly . need % gdp spend cap federal level (...</td>\n",
       "      <td>exactly need gap spending cap federal level ei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256049</th>\n",
       "      <td>By your own comment, even if some of them vote...</td>\n",
       "      <td>comment , even vote ndp pq , trudeau demonstra...</td>\n",
       "      <td>comment even vote nip pm trudeau demonstrably ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  \\\n",
       "286892   What a breathe of fresh air to have someone wh...   \n",
       "419218   Your jewish friends were the ones who told you...   \n",
       "1055330  Possible collusion by Trump and his affiliates...   \n",
       "1382764  Exactly.  We need a % of GDP spending cap at t...   \n",
       "256049   By your own comment, even if some of them vote...   \n",
       "\n",
       "                                                text_light  \\\n",
       "286892   breathe fresh air someone embrace common sense...   \n",
       "419218   jewish friend one tell zionist control canada ...   \n",
       "1055330  possible collusion trump affiliate debunk , st...   \n",
       "1382764  exactly . need % gdp spend cap federal level (...   \n",
       "256049   comment , even vote ndp pq , trudeau demonstra...   \n",
       "\n",
       "                                           text_aggressive  \n",
       "286892   breathe fresh air someone embrace common sense...  \n",
       "419218   jewish friend one tell zionists control canada...  \n",
       "1055330  possible collusion trump affiliate debunk stat...  \n",
       "1382764  exactly need gap spending cap federal level ei...  \n",
       "256049   comment even vote nip pm trudeau demonstrably ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Sanity check: very small sample ---\n",
    "N_SANITY = 100\n",
    "\n",
    "df_sanity = df_train.sample(N_SANITY, random_state=42).copy()\n",
    "\n",
    "df_sanity[\"text_light\"] = df_sanity[\"text\"].apply(light)\n",
    "df_sanity[\"text_aggressive\"] = df_sanity[\"text\"].apply(aggressive)\n",
    "\n",
    "df_sanity[[\"text\", \"text_light\", \"text_aggressive\"]].head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06c79145",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#df_train[\"text_light\"] = df_train[\"text\"].apply(light)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#df_train[\"text_aggressive\"] = df_train[\"text\"].apply(aggressive)\u001b[39;00m\n\u001b[32m      4\u001b[39m df_train_small[\u001b[33m\"\u001b[39m\u001b[33mtext_light\u001b[39m\u001b[33m\"\u001b[39m] = df_train_small[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m].apply(light)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df_train_small[\u001b[33m\"\u001b[39m\u001b[33mtext_aggressive\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mdf_train_small\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43maggressive\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\toxic-ml\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:4943\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4808\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4809\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4810\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4815\u001b[39m     **kwargs,\n\u001b[32m   4816\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4817\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4818\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4819\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4934\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4935\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4936\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4937\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4938\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4939\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4941\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4943\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\toxic-ml\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\toxic-ml\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\toxic-ml\\.venv\\Lib\\site-packages\\pandas\\core\\base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\toxic-ml\\.venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36maggressive\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34maggressive\u001b[39m(text):\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\toxic-ml\\notebooks\\../src\\preprocessing.py:150\u001b[39m, in \u001b[36mpreprocess\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m    148\u001b[39m text = emoticons.transform(text)                 \u001b[38;5;66;03m# Convert emoticons to words\u001b[39;00m\n\u001b[32m    149\u001b[39m text = emojis.transform(text)                    \u001b[38;5;66;03m# Convert emojis to words\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m text = \u001b[43mabbreviations\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m             \u001b[38;5;66;03m# Convert slang abbreviations\u001b[39;00m\n\u001b[32m    151\u001b[39m text = unidecode(text)                           \u001b[38;5;66;03m# Convert the rest to ASCII\u001b[39;00m\n\u001b[32m    152\u001b[39m text = contractions.fix(text)                    \u001b[38;5;66;03m# Expand contractions\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\toxic-ml\\notebooks\\../src\\preprocessing.py:40\u001b[39m, in \u001b[36mRegexProcessor.transform\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m''' Converts all patterns(keys) to their corresponding values '''\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dict.items():\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     text = key.sub(value, text)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#df_train[\"text_light\"] = df_train[\"text\"].apply(light)\n",
    "#df_train[\"text_aggressive\"] = df_train[\"text\"].apply(aggressive)\n",
    "\n",
    "df_train_small[\"text_light\"] = df_train_small[\"text\"].apply(light)\n",
    "df_train_small[\"text_aggressive\"] = df_train_small[\"text\"].apply(aggressive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56be9579",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_small[\"text_light\"] = df_train_small[\"text\"].apply(light)\n",
    "df_test_small[\"text_aggressive\"] = df_train_small[\"text\"].apply(aggressive)\n",
    "df_valid_small[\"text_light\"] = df_train_small[\"text\"].apply(light)\n",
    "df_valid_small[\"text_aggressive\"] = df_train_small[\"text\"].apply(aggressive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b358304",
   "metadata": {},
   "source": [
    "## Comparison of preprocessing strategies\n",
    "\n",
    "We compare the two pipelines using text length statistics and qualitative examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7456f1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_small[\"len_light\"] = df_train_small[\"text_light\"].str.split().apply(len)\n",
    "df_train_small[\"len_aggr\"]  = df_train_small[\"text_aggressive\"].str.split().apply(len)\n",
    "\n",
    "df_train_small[[\"len_light\", \"len_aggr\"]].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d67c9c",
   "metadata": {},
   "source": [
    "## Qualitative comparison: raw vs light vs aggressive preprocessing\n",
    "\n",
    "To better understand the impact of preprocessing, we display a concrete example\n",
    "of a raw comment alongside its light (xLSTM-style) and aggressive cleaned versions.\n",
    "This qualitative comparison highlights how different preprocessing choices\n",
    "transform the same input text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de1e341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one example comment\n",
    "example_text = df_train.loc[0, \"text\"]\n",
    "\n",
    "# Apply both preprocessing pipelines\n",
    "example_light = light(example_text)\n",
    "example_aggressive = aggressive(example_text)\n",
    "\n",
    "print(\"RAW COMMENT:\")\n",
    "print(example_text)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100 + \"\\n\")\n",
    "\n",
    "print(\"LIGHT PREPROCESSING (xLSTM-style):\")\n",
    "print(example_light)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100 + \"\\n\")\n",
    "\n",
    "print(\"AGGRESSIVE PREPROCESSING:\")\n",
    "print(example_aggressive)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9864c7c5",
   "metadata": {},
   "source": [
    "## Export processed datasets\n",
    "\n",
    "We export two versions:\n",
    "- `civil_comments_light_xlstm.(csv|parquet)`\n",
    "- `civil_comments_aggressive.(csv|parquet)`\n",
    "\n",
    "They are stored in `data/processed/` and excluded from Git versioning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ac04db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_small.to_csv(\"../data/processed/train_cleaned.csv\", index=False)\n",
    "df_valid_small.to_csv(\"../data/processed/valid_cleaned.csv\", index=False)\n",
    "df_test_small.to_csv(\"../data/processed/test_cleaned.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da374772",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook compared light and aggressive preprocessing strategies.\n",
    "The resulting datasets will be used to train and evaluate multiple models,\n",
    "allowing us to assess the impact of preprocessing choices on performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
